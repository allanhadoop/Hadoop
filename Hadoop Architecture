# Source - https://www.youtube.com/watch?v=ECxlFPy7s-k

# source - https://www.youtube.com/playlist?list=PLkz1SCf5iB4dw3jbRo0SYCk2urRESUA3v


Hadoop cluster is master and slave nodes. Combination of one set of master+slaves is called rack. Rack has switch ,if that fails then whole 
rack will be down. 

Client - This is requester who wants to communicate to HDFS filesystem which is categorized into name and data node
Master node is name node (it manages names of directories and files) 
Slave node is data node (it manages data of file)

# # see attached pdf file for more detail 

# HDFS interfaces to perform file operation on hdfs
1. command line
2. Apache Hue
3. Java API
4. Rest API webhdfs
5. C Api libhdfs


Hadoop single node cluster includes - name node, datanode and hadoop client .while working on large multi-node cluster , access to 
name node or data node may be restricted due to security reasons , so there are few edge nodes which can be used for hdfs operations
edge nodes are like data nodes which can be used for day to day operations.

# hdfs commands - https://www.youtube.com/watch?v=PI1Xm1Zoj78
hdfs version
start-all.sh

## HDFS Compatible file systems are as follows -- 
1. AWS s3
2. Azure block storage
3. Azure data lake storage
4. swift storage
5. Linux local file system 

# How to access Hadoop supported file system 
Access it using URI - scheme://authority/path
To access local file system - file://
To access HDFS - hdfs://namenode (e.g. hdfs://<default file syste>/path
To get the default file system , use below command
>> hdfs getconf -confkey fs.defaultFS
This may output as hdfs://localhost:9000

## How to bring large data into HDFS 
1. cp
2. copyFromLocal
3. Put
4. appendTofFile

cp - example to copy local file into hdfs
hadoop fs -cp file:///home/file1.txt file:///home/file2.txt /user/hduser/test    # copy file1/file2 into test folder

### Mapreduce framework and programming technique
submit Map Reduce jar to Apache Yarn which will execute MapReduce framework












