1. pre-requisite for hadoop 
2. Install hadoop 

# source - https://www.youtube.com/watch?v=f6cuNc5S7L0
# install java first 
java -version 

sudo addgroup hadoop
sudo adduser hduser
sudo adduser hduser hadoop

sudo visudo
hduser ALL=(ALL)ALL

#swtich to hduser & install ssh ( hadoop nodes talk to each other by using ssh)
sudo apt-get install openssh-server
ssh-keygen
cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys
chmod 700 ~/.ssh/authorized_keys

sudo service ssh status 
sudo /etc/init.d/ssh restart 
ssh localhost    # check ssh works .. system should not ask for password

#hadoop and ipv6 dont agree on 0.0.0.0 . so disable that
# disable ipv6 
net.ipv6.conf.all.disable_ipv6 = 1
net.ipv6.conf.default.disable_ipv6 = 1
net.ipv6.conf.lo.disable_ipv6 = 1
    
#check if ipv6 is disabled  (output of below command should be 0 and after reboot it should show 1)
cat /proc/sys/net/ipv6/conf/all/disable_ipv6

##### Install hadoop 
#download hadoop 2.7.x from apache website
move file to /usr/local
sudo mv ~/Download/hadoop_2.7.gz ~/usr/local/
sudo tar -xvf hadoop_2.7.gz
sudo ln -s hadoop_2.7 hadoop 
sudo chown -R hduser:hadoop hadoop_2.7
sudo chmod 777 hadoop_2.7

--add below 3 lines into hadoop
sudo vim /usr/local/hadoop/etc/hadoop/hadoop-env.sh
export HADOOP_OPTS=-Djava.net.preferIPv4Stack=true
export HADOOP_HOME_wARN_SUPPRESS="TRUE"
export JAVA_HOME=/usr/local/java/jdk1.8.0_91

--add below lines into ~/.bashrc
sudo vim ~/.bashrc
export HADOOP_HOME=/usr/local/hadoop
export HADOOP_PREFIX=/usr/local/hadoop
export HADOOP_MAPRED_HOME=${HADOOP_HOME}
export HADOOP_COMMON_HOME=${HADOOP_HOME}
export HADOOP_HDFS_HOME=${HADOOP_HOME}
export HADOOP_YARN_HOME=${HADOOP_HOME}
export HADOOP_CONF_DIR=${HADOOP_HOME}/etc/hadoop

#Native path
export HADOOP_COMMON_LIB_NATIVE_DIR=${HADOOP_PREFIX}/lib/native
export HADOOP_OPTS="-Djava.library.path=$HADOOP_PREFIX/lib"

#set JAVA_HOME (we will also configure JAVA_HOME directly for hadoop later on)
export JAVA_HOME=/usr/local/java/jdk1.8.0_91
# some convenient aliases and functions for running hadoop-related commands

unaliasfs&> /dev/null
aliasfs="hadoop fs"
unaliasls&> /dev/null
aliasshls="fs -ls"

export PATH=$PATH:$HADOOP_HOME/bin:$PATH:$JAVA_HOME/bin:$HADOOP_HOME/sbin

# make temp directory
sudo  mkdir -p /app/hadoop/tmp
sudo chown -R hduser:hadoop /app/hadoop/tmp
sudo chmod -R 777 /app/hadoop/tmp

vim /usr/local/hadoop/etc/hadoop/yarn-site.xml
<property> 
   <name>yarn.nodemanager.aux-services</name>
   <value>mapreduce.shuffle</value>
</property>
<property>
   <name>yarn.nodemanager.aux-services.mapreduce.shuffle.class</name>
   <value>org.apache.hadoop.mapred.ShuffleHandler</value>
</property>

vim /usr/local/hadoop/etc/hadoop/core-site.xml
<property>
<name>hadoop.tmp.dir</name>
<value>/app/hadoop/tmp</value>
<description>Base for other temporary directory</description>
</property>

<property>
<name>fs.default.name</name>
<value>hdfs://localhost:9000</value>
<description>the name of the default file system 
A URI whose scheme and authority determine the filesystem implementation
the URIs's scheme determines the config property. The URI's authority determines host , port etc</description>
</property>

#copy mapred-site.xml.template to mapred-site.xml
cp /usr/local/hadoop/etc/hadoop/mapred-site.xml.template /usr/local/hadoop/etc/hadoop/mapred-site.xml
vim mapred-site.xml
<property>
  <name>mapreduce.framework.name</name>
  <value>yarn</value>
</property>

#make few directory for HDFS 
sudo mkdir -p /usr/local/hadoop/yarn_data/hdfs/namenode
sudo mkdir -p /usr/local/hadoop/yarn_data/hdfs/datanode

sudo chmod 777 /usr/local/hadoop/yarn_data/hdfs/namenode
sudo chmod 777 /usr/local/hadoop/yarn_data/hdfs/datanode

sudo chown -R hduser:hadoop /usr/local/hadoop/yarn_data/hdfs/namenode
sudo chown -R hduser:hadoop /usr/local/hadoop/yarn_data/hdfs/datanode

vim /usr/local/hadoop/etc/hadoop/hdfs-site.xml
<property>
  <name>dfs.replication</name>
  <value>1</value>
</property>
<property>
  <name>dfs.namenode.name.dir</name>
  <value>file:/usr/local/hadoop/yarn_data/hdfs/namenode</value>
</property>
<property>
  <name>dfs.datanode.data.dir</name>
  <value>file:/usr/local/hadoop/yarn_data/hdfs/datanode</value>
</property>











